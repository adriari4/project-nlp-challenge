{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "Introduction\n",
    "Learning how to process text is a skill required for Data Scientists/AI Engineers.\n",
    "In this project, you will put these skills into practice to identify whether a news headline is real or fake news.\n",
    "\n",
    "Project Overview\n",
    "In the file dataset/data.csv, you will find a dataset containing news articles with the following columns:\n",
    "\n",
    "- label: 0 if the news is fake, 1 if the news is real.\n",
    "- title: The headline of the news article.\n",
    "- text: The full content of the article.\n",
    "- subject: The category or topic of the news.\n",
    "- date: The publication date of the article.\n",
    "- Your goal is to build a classifier that is able to distinguish between the two.\n",
    "\n",
    "Once you have a classifier built, then use it to predict the labels for dataset/validation_data.csv. Generate a new file where the label 2 has been replaced by 0 (fake) or 1 (real) according to your model. Please respect the original file format, do not include extra columns, and respect the column separator.\n",
    "\n",
    "Please ensure to split the data.csv into training and test datasets before using it for model training or evaluation.\n",
    "\n",
    "Guidance\n",
    "Like in a real life scenario, you are able to make your own choices and text treatment. Use the techniques you have learned and the common packages to process this data and classify the text.\n",
    "\n",
    "Deliverables\n",
    "- Python Code: Provide well-documented Python code that conducts the analysis.\n",
    "- Predictions: A csv file in the same format as validation_data.csv but with the predicted labels (0 or 1)\n",
    "- Accuracy estimation: Provide the teacher with your estimation of how your model will perform.\n",
    "- Presentation: You will present your model in a 10-minute presentation. Your teacher will provide further instructions.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                              title  \\\n",
      "0      1  As U.S. budget fight looms, Republicans flip t...   \n",
      "1      1  U.S. military to accept transgender recruits o...   \n",
      "2      1  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3      1  FBI Russia probe helped by Australian diplomat...   \n",
      "4      1  Trump wants Postal Service to charge 'much mor...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
      "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
      "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
      "\n",
      "                 date  \n",
      "0  December 31, 2017   \n",
      "1  December 29, 2017   \n",
      "2  December 31, 2017   \n",
      "3  December 30, 2017   \n",
      "4  December 29, 2017   \n",
      "Index(['label', 'title', 'text', 'subject', 'date'], dtype='object')\n",
      "(39942, 5)\n",
      "label      0\n",
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1) Load & quick sanity checks\n",
    "# Goal: check head, confirm columns, size, nulls, class balance.\n",
    "import pandas as pd\n",
    "data_train = pd.read_csv('dataset/data.csv')\n",
    "data_validation = pd.read_csv('dataset/validation_data.csv')\n",
    "print(data_train.head())\n",
    "print(data_train.columns)\n",
    "print(data_train.shape)\n",
    "print(data_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31953, 4) (7989, 4) (31953,) (7989,)\n"
     ]
    }
   ],
   "source": [
    "# 2) Train/test split (from data.csv)\n",
    "# Use stratified split on label to preserve class balance.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# features weâ€™ll use\n",
    "X = data_train[['title','text','subject','date']]\n",
    "# target variable\n",
    "y = data_train['label'].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Choose features (text) & minimal preprocessing.\n",
    "# Easiest strong baseline: TF-IDF combining title + text.\n",
    "for split in [X_train, X_test]:\n",
    "    split['combined_text'] = split['title'].fillna('') + ' ' + split['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Vectorizer + model in a Pipeline\n",
    "# Start with TfidfVectorizer + LogisticRegression (fast, strong baseline). You can swap in LinearSVC or MultinomialNB later.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    # converting raw text into numerical features\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        # lower case all text\n",
    "        lowercase= True,\n",
    "        # remove common words like the, is, and\n",
    "        stop_words = \"english\",\n",
    "        # limit vocabulary size & keep only the 100,000 most frequent terms\n",
    "        max_features=100_000,\n",
    "        # use both single words (unigrams) and pairs of words (bigrams)\n",
    "        ngram_range=(1,2),\n",
    "        # ignore words that appear in fewer than 2 documents\n",
    "        min_df=2\n",
    "    )),\n",
    "    # machine learning model to classify news as fake (0) or real (1)\n",
    "    (\"clf\", LogisticRegression(\n",
    "        # algorithm to not stop until 200 iterations\n",
    "        max_iter=200,\n",
    "        n_jobs=None,\n",
    "        # automatically balance weights for classes\n",
    "        class_weight='balanced'\n",
    "     ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9938033987419022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9960    0.9915    0.9938     15954\n",
      "           1     0.9916    0.9961    0.9938     15999\n",
      "\n",
      "    accuracy                         0.9938     31953\n",
      "   macro avg     0.9938    0.9938    0.9938     31953\n",
      "weighted avg     0.9938    0.9938    0.9938     31953\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5) Fit & evaluate (baseline)\n",
    "# Use accuracy and precision/recall/F1 (binary classification).\n",
    "pipeline.fit(X_train['combined_text'], y_train)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "y_train_pred = pipeline.predict(X_train['combined_text'])\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(classification_report(y_train, y_train_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9938033987419022\n",
      "Test Accuracy: 0.9854800350481913\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9889    0.9820    0.9854      3989\n",
      "           1     0.9821    0.9890    0.9856      4000\n",
      "\n",
      "    accuracy                         0.9855      7989\n",
      "   macro avg     0.9855    0.9855    0.9855      7989\n",
      "weighted avg     0.9855    0.9855    0.9855      7989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for overfitting\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_train_pred = pipeline.predict(X_train['combined_text'])\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "y_test_pred = pipeline.predict(X_test['combined_text'])\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Train on full training data (optional)\n",
    "# After choosing final settings (pipe or best_model), you can refit on all of df for maximal signal before producing validation predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Produce predictions for validation_data.csv\n",
    "# The file format must be the same as the original, but with label 2 replaced by your predictions (0/1).\n",
    "# Usually validation_data.csv has label=2 as a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Accuracy estimation (what to report)\n",
    "# Report test set metrics from step 5 (or your CV estimates).\n",
    "# State: test accuracy, precision/recall/F1 for each class, and any notable error patterns (e.g., satire mistaken as fake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) (Optional) Nice upgrades\n",
    "# Use subject: combine with text via ColumnTransformer.\n",
    "# Use date: extract year/month; sometimes correlates with patterns.\n",
    "# Calibrate probabilities (CalibratedClassifierCV) if you want threshold tuning.\n",
    "# Error analysis: inspect top false positives/negatives to refine preprocessing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
